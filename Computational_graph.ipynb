{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b84153-b7d6-4822-a1da-433f28270eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Step 1: Define tensors with 'requires_grad=True' to track gradients\n",
    "a = torch.tensor([2.0], requires_grad=True)  # Leaf node in the computational graph\n",
    "b = torch.tensor([1.0], requires_grad=True)  # Leaf node in the computational graph\n",
    "\n",
    "# Print the initial tensors\n",
    "print(f\"Initial Tensors:\\na = {a.item()}, b = {b.item()}\\n\")\n",
    "\n",
    "# Step 2: Perform operations to build the computational graph\n",
    "c = a + b  # Intermediate node (not a leaf node)\n",
    "d = b + 1  # Intermediate node (not a leaf node)\n",
    "e = c * d  # Final result (output)\n",
    "\n",
    "# Retain gradients for intermediate variables\n",
    "c.retain_grad()  # Not required for leaf nodes, only for intermediate nodes\n",
    "d.retain_grad()  # Retaining gradients for 'd'\n",
    "e.retain_grad()  # Retaining gradients for 'e'\n",
    "\n",
    "# Print the intermediate results\n",
    "print(f\"Intermediate Results:\\nc = a + b = {c.item()}\\nd = b + 1 = {d.item()}\\ne = c * d = {e.item()}\\n\")\n",
    "\n",
    "# Step 3: Backpropagate to compute gradients\n",
    "e.backward()  # Compute gradients with respect to 'e'\n",
    "\n",
    "# Step 4: Display gradients\n",
    "print(\"Gradients after backpropagation:\")\n",
    "print(f\"a.grad = {a.grad.item()}\")  # Gradient of 'e' with respect to 'a'\n",
    "print(f\"b.grad = {b.grad.item()}\")  # Gradient of 'e' with respect to 'b'\"\n",
    "print(f\"c.grad = {c.grad.item()}\")  # Gradient of 'e' with respect to 'c'\n",
    "print(f\"d.grad = {d.grad.item()}\")  # Gradient of 'e' with respect to 'd'\"\n",
    "print(f\"e.grad = {e.grad.item()}\")         # 'e' is a scalar, so its gradient is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b1d31-b250-4a5f-84d6-60bc385b8bd9",
   "metadata": {},
   "source": [
    "### Key Explanations\n",
    "\n",
    "1. **Leaf Nodes (`a` and `b`)**:  \n",
    "   These are the initial tensors where we set `requires_grad=True`. PyTorch tracks all operations involving these tensors to enable automatic differentiation.\n",
    "\n",
    "2. **Intermediate Nodes (`c` and `d`)**:  \n",
    "   These are results of operations (`a + b` and `b + 1` respectively). While gradients are not retained by default for intermediate nodes, we explicitly call `retain_grad()` for demonstration.\n",
    "\n",
    "3. **The Computational Graph**:  \n",
    "   PyTorch dynamically constructs a computational graph behind the scenes. Each operation (e.g., addition, multiplication) creates new nodes in the graph, linking inputs to outputs. When we call `backward()` on the final result (`e`), gradients are propagated back through the graph.\n",
    "\n",
    "4. **Gradient Calculation**:\n",
    "   - `e.backward()` computes the gradient of `e` with respect to `a`, `b`, `c`, and `d`.\n",
    "   - Leaf nodes (`a` and `b`) have their gradients populated, and we can print them using `a.grad` and `b.grad`.\n",
    "   - Intermediate nodes (`c` and `d`) also have gradients, which we access via `c.grad` and `d.grad` (because we retained these gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796e667-618d-4251-82e2-e73bf29be38e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
